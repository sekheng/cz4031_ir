{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading vader_lexicon: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1131)>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import snscrape.modules.twitter as twitter\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import pysolr\n",
    "import uuid\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"vader_lexicon\")\n",
    "def crawl_from_twitter(cnt, query_term):\n",
    "    sent_analyser = SentimentIntensityAnalyzer()\n",
    "    tweets = twitter.TwitterSearchScraper(query_term).get_items()\n",
    "    sliced_scraped_tweets = itertools.islice(tweets, 10)\n",
    "    solr = pysolr.Solr(\n",
    "        'http://localhost:8983/solr/stock_project_core', always_commit=True)\n",
    "    tweets_to_add = []\n",
    "    for tweet in sliced_scraped_tweets:\n",
    "        sentiment = 0\n",
    "        if sent_analyser.polarity_scores(tweet.rawContent)[\"compound\"]>=0:\n",
    "            sentiment = 1\n",
    "        else:\n",
    "            sentiment = 0\n",
    "        temp = {\n",
    "            \"body\": [tweet.rawContent],\n",
    "            \"id\": str(uuid.uuid4()),\n",
    "            \"like_num\": tweet.likeCount,\n",
    "            \"sentiment\": sentiment,\n",
    "            \"ticker_symbol\": query_term,\n",
    "            \"tweet_id\": tweet.id,\n",
    "            \"post_date\": str(tweet.date),\n",
    "            \"retweet_num\": tweet.retweetCount,\n",
    "            \"writer\": tweet.user.username,\n",
    "        }\n",
    "        tweets_to_add.append(temp)\n",
    "    solr.add(tweets_to_add)\n",
    "    return \n",
    "crawl_from_twitter(10, \"tesla\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A   b\n",
       "0  0  -1\n",
       "1  1   0\n",
       "2  2   1\n",
       "3  3   2\n",
       "4  4   3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['A','b'])\n",
    "\n",
    "for i in range(5):\n",
    "    new_row = pd.Series({'A': i, 'b': i-1})\n",
    "    df=pd.concat([df, new_row.to_frame().T], ignore_index=True)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# , parallel_backend, register_parallel_backend\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import contractions\n",
    "import textstat\n",
    "import emoji as Emoji\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('vader_lexicon')\n",
    "stop = stopwords.words('english')\n",
    "# Parallel processing\n",
    "\n",
    "\n",
    "class SubPreprocessor:\n",
    "    def __init__(self, stemmer=PorterStemmer()):\n",
    "        self.stemmer = stemmer\n",
    "        #self.tokenize = tokenize\n",
    "        self.stop_words = stop\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        res = Parallel(n_jobs=-1)(\n",
    "            delayed(self.processRow)(row[0]) for row in X.loc[:, 'raw_text']\n",
    "        )\n",
    "        res = pd.DataFrame(res, index=X.index)\n",
    "        res = pd.concat([X, res], axis=1)\n",
    "        return res\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        return self.fit(X, y).transform(X)\n",
    "\n",
    "    def processRow(self, text):\n",
    "        text, urls = self.extractLink(text, '')\n",
    "        text, emojis = self.extractEmoji(text, '')\n",
    "        text, emoticons = self.extractEmoticons(text, '')\n",
    "        text = self.removePunctuation(text)\n",
    "        text = self.removeRtLink(text)\n",
    "        text = self.removeStopWords(text)\n",
    "\n",
    "        return {\n",
    "            #             'url_cnt': len(urls),\n",
    "            #             'emoticons': emoticons,\n",
    "            #             'emojis': emojis ,\n",
    "            #             'emo_cnt': len(emoticons) + sum(emojis.values()),\n",
    "            'clean_text_no_stem_user': text,\n",
    "        }\n",
    "\n",
    "    def removePunctuation(self, text, replace='', remove_num=False, remove_emoji=False):\n",
    "        r_emoji = '\\u00a9|\\u00ae|[\\u2000-\\u3300]|\\ud83c[\\ud000-\\udfff]|\\ud83d[\\ud000-\\udfff]|\\ud83e[\\ud000-\\udfff]'\n",
    "        r = f'[!#\"$%&\\'()*+,-./:;<=>?@[\\]^_`{{|}}~—]'\n",
    "        if remove_emoji:\n",
    "            r += f'|[{r_emoji}]'\n",
    "        if remove_num:\n",
    "            r += f'|[0-9]'\n",
    "        text = re.sub(r, replace, text)\n",
    "        return text\n",
    "\n",
    "    def extractLink(self, text, replace_text=''):\n",
    "        #r = '(http\\S+?|www.\\S+?)(?=\\'|\\\")'\n",
    "        #r = '(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b'\n",
    "        #r = '''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))'''\n",
    "        #r ='^https?:\\/\\/.*[\\r\\n]*'\n",
    "        r = 'http://\\S+|https://\\S+'\n",
    "        # .apply(lambda url: url[1:-1]) # trim quotes\n",
    "        urls = re.findall(r, text)\n",
    "        text = re.sub(r, replace_text, text)\n",
    "        return text, urls\n",
    "\n",
    "    def extractEmoticons(self, text, replace_text=''):\n",
    "        r = '(?::|;|=|X)(?:-)?(?:\\)|\\(|D|P)'\n",
    "        emoticons = re.findall(r, text)\n",
    "        text = re.sub(r, replace_text, text)\n",
    "        # replace('-','') removes nose of emoticons\n",
    "        emoticons = [emoticon.replace('-', '') for emoticon in emoticons]\n",
    "        return text, emoticons\n",
    "\n",
    "    def extractEmoji(self, text, replace_text=''):\n",
    "        distinct_ls = Emoji.distinct_emoji_list(text)\n",
    "        emoji_cnt = dict(zip(distinct_ls, [0 for i in distinct_ls]))\n",
    "        for emoji in emoji_cnt.keys():\n",
    "            emoji_cnt[emoji] = text.count(emoji)\n",
    "            text = text.replace(emoji, replace_text)\n",
    "\n",
    "        return text, emoji_cnt\n",
    "\n",
    "    def wordCount(self, text):\n",
    "        return textstat.lexicon_count(text, removepunct=True)\n",
    "\n",
    "    def splitWords(self, text):\n",
    "        w_list = re.split('\\s+', text.strip())\n",
    "        return w_list\n",
    "\n",
    "    def removeRtLink(self, text):\n",
    "        # Removes RT\n",
    "        #text = re.sub('RT @\\w+: ','', text)\n",
    "        #text = text.lower()\n",
    "\n",
    "        # Removes @username from the tweet\n",
    "        #text = re.sub(r'(@[A-Za-z0-9_]+)', '', text)\n",
    "\n",
    "        # Removes link\n",
    "        text = re.sub('http://\\S+|https://\\S+', '', text)\n",
    "\n",
    "        # Only considers string or digits or whitespace\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "        # Removes digits\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        return text\n",
    "\n",
    "    def removeStopWords(self, text):\n",
    "        #text_tokens = self.tokenize(text)\n",
    "        text_tokens = word_tokenize(text)\n",
    "        text = [word for word in text_tokens if not word in self.stop_words]\n",
    "        return ' '.join(text)\n",
    "\n",
    "    def __call__(self, text):\n",
    "        return self.processRow(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>company</th>\n",
       "      <th>post_date</th>\n",
       "      <th>author</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>like_num</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean_text_no_stem_user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1642345902303219713</td>\n",
       "      <td>apple</td>\n",
       "      <td>2023-04-02 01:59:11+00:00</td>\n",
       "      <td>atomiccanonAPPL</td>\n",
       "      <td>[グレビュオフのコスプレできるやん！！！]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>グレビュオフのコスプレできるやん</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id company                  post_date           author   \n",
       "0  1642345902303219713   apple  2023-04-02 01:59:11+00:00  atomiccanonAPPL  \\\n",
       "\n",
       "                raw_text like_num subjectivity sentiment   \n",
       "0  [グレビュオフのコスプレできるやん！！！]        0            0         1  \\\n",
       "\n",
       "  clean_text_no_stem_user  \n",
       "0        グレビュオフのコスプレできるやん  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(columns=['id', 'company', 'post_date', 'author',\n",
    "                           'raw_text', 'like_num', 'subjectivity', 'sentiment'])\n",
    "temp = {'id': 1642345902303219713, 'company': 'apple', 'post_date': '2023-04-02 01:59:11+00:00',\n",
    "        'author': 'atomiccanonAPPL', 'raw_text': ['グレビュオフのコスプレできるやん！！！'], 'like_num': 0, 'subjectivity': 0, 'sentiment': 1}\n",
    "df = pd.concat(\n",
    "    [df, pd.DataFrame([temp], columns=df.columns)], ignore_index=True)\n",
    "prep = SubPreprocessor()\n",
    "df = prep.transform(df)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import keras\n",
    "\n",
    "# load the model from file\n",
    "from keras.models import load_model\n",
    "model = load_model('model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2023', '01', '01']\n"
     ]
    }
   ],
   "source": [
    "d1= \"2023-01-01\"\n",
    "t = d1.split(\"-\")\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'q': 'company:AAPL AND subjectivity:1', 'fq': 'post_date:[2023-01-01T00:00:00Z TO 2023-03-30T00:00:00Z]', 'rows': 0}\n",
      "{'q': 'company:AAPL AND subjectivity:0', 'fq': 'post_date:[2023-01-01T00:00:00Z TO 2023-03-30T00:00:00Z]', 'rows': 0}\n",
      "{'q': 'company:AAPL AND sentiment:0', 'fq': 'post_date:[2023-01-01T00:00:00Z TO 2023-03-30T00:00:00Z]', 'rows': 0}\n",
      "{'q': 'company:AAPL AND sentiment:1', 'fq': 'post_date:[2023-01-01T00:00:00Z TO 2023-03-30T00:00:00Z]', 'rows': 0}\n",
      "{'total': 892, 'subjective': 440, 'non_subjective': 452, 'negative': 515, 'neutral': 172, 'positive': 205, 'non_subjective_negative': 270, 'non_subjective_neutral': 172, 'non_subjective_positive': 10, 'subjective_negative': 245, 'subjective_neutral': 0, 'subjective_positive': 195}\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "solr = pysolr.Solr(\n",
    "    'http://localhost:8983/solr/stock_project_core', always_commit=True)\n",
    "start_date = datetime.datetime(2023, 1, 1)\n",
    "end_date = datetime.datetime(2023, 3, 30)\n",
    "# Convert the dates to Solr date format\n",
    "start_date_str = start_date.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "end_date_str = end_date.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "range_query = 'post_date:[{} TO {}]'.format(start_date_str, end_date_str)\n",
    "t = \"company:AAPL\"\n",
    "normal_params = {\n",
    "    'q': t,\n",
    "    'fq': range_query,\n",
    "    'rows': 0,\n",
    "}\n",
    "results = solr.search(**normal_params)\n",
    "total_tweets = results.hits\n",
    "\n",
    "subjective_params = {\n",
    "    'q': t + \" AND subjectivity:1\",\n",
    "    'fq': range_query,\n",
    "    'rows': 0,\n",
    "}\n",
    "print(subjective_params)\n",
    "results = solr.search(**subjective_params)\n",
    "subjective_tweets = results.hits\n",
    "\n",
    "non_subjective_params = {\n",
    "    'q': t + \" AND subjectivity:0\",\n",
    "    'fq': range_query,\n",
    "    'rows': 0,\n",
    "}\n",
    "print(non_subjective_params)\n",
    "results = solr.search(**non_subjective_params)\n",
    "non_subjective_tweets = results.hits\n",
    "\n",
    "neutral_sentiment_params = {\n",
    "    'q': t + \" AND sentiment:0\",\n",
    "    'fq': range_query,\n",
    "    'rows': 0,\n",
    "}\n",
    "print(neutral_sentiment_params)\n",
    "results = solr.search(**neutral_sentiment_params)\n",
    "neutral_tweets = results.hits\n",
    "\n",
    "positive_sentiment_params = {\n",
    "    'q': t + \" AND sentiment:1\",\n",
    "    'fq': range_query,\n",
    "    'rows': 0,\n",
    "}\n",
    "\n",
    "results = solr.search(**positive_sentiment_params)\n",
    "positive_tweets = results.hits\n",
    "\n",
    "negative_tweets = total_tweets - positive_tweets - neutral_tweets\n",
    "\n",
    "non_sub_neu_params = {\n",
    "    'q': t + \" AND subjectivity:0 AND sentiment:0\",\n",
    "    'fq': range_query,\n",
    "    'rows': 0,\n",
    "}\n",
    "results = solr.search(**non_sub_neu_params)\n",
    "non_subjective_neutral = results.hits\n",
    "\n",
    "non_sub_pos_params = {\n",
    "    'q': t + \" AND subjectivity:0 AND sentiment:1\",\n",
    "    'fq': range_query,\n",
    "    'rows': 0,\n",
    "}\n",
    "results = solr.search(**non_sub_pos_params)\n",
    "non_subjective_positive = results.hits\n",
    "non_subjective_negative = non_subjective_tweets - non_subjective_positive - non_subjective_neutral\n",
    "\n",
    "sub_neu_params = {\n",
    "    'q': t + \" AND subjectivity:1 AND sentiment:0\",\n",
    "    'fq': range_query,\n",
    "    'rows': 0,\n",
    "}\n",
    "results = solr.search(**sub_neu_params)\n",
    "subjective_neutral = results.hits\n",
    "\n",
    "sub_pos_params = {\n",
    "    'q': t + \" AND subjectivity:1 AND sentiment:1\",\n",
    "    'fq': range_query,\n",
    "    'rows': 0,\n",
    "}\n",
    "results = solr.search(**sub_pos_params)\n",
    "subjective_positive = results.hits\n",
    "\n",
    "subjective_negative = subjective_tweets - subjective_positive - subjective_neutral\n",
    "\n",
    "response = {\n",
    "    \"total\":total_tweets,\n",
    "    \"subjective\": subjective_tweets,\n",
    "    \"non_subjective\": non_subjective_tweets,\n",
    "    \"negative\": negative_tweets,\n",
    "    \"neutral\": neutral_tweets,\n",
    "    \"positive\": positive_tweets,\n",
    "    \"non_subjective_negative\": non_subjective_negative,\n",
    "    \"non_subjective_neutral\": non_subjective_neutral,\n",
    "    \"non_subjective_positive\": non_subjective_positive,\n",
    "    \"subjective_negative\": subjective_negative,\n",
    "    \"subjective_neutral\": subjective_neutral,\n",
    "    \"subjective_positive\": subjective_positive\n",
    "}\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
